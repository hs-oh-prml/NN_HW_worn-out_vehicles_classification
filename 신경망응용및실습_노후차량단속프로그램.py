# -*- coding: utf-8 -*-
"""ì‹ ê²½ë§ì‘ìš©ë°ì‹¤ìŠµ_ë…¸í›„ì°¨ëŸ‰ë‹¨ì†í”„ë¡œê·¸ë¨.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/12ekawDlL2R9B_CjmgbrTfokoVOKAKtYK

<img src='https://encrypted-tbn0.gstatic.com/images?q=tbn:ANd9GcSLeCkQvV4ihJxtxPIkhiTaTyUi2NniNjhrZg&usqp=CAU' width=120%>


# ğŸ”¥ **2021-2 ì‹ ê²½ë§ì‘ìš© ë° ì‹¤ìŠµ í”„ë¡œì íŠ¸** ğŸ”¥ 
<font size=5><b>ì„œìš¸ì‹œ ë…¸í›„ì°¨ëŸ‰ ë‹¨ì† í”„ë¡œê·¸ë¨<b></font>
<div align='right'>ì¶œì œì:ë¥˜ íšŒ ì„±<br>(hoesungryu@korea.ac.kr)</div>


<hr style="height:3px;border:none;color:#5F71F7;background-color:#5F71F7;" />

## ğŸ“Œ Description

ìµœê·¼ ì„œìš¸ì‹œëŠ” ì €ê³µí•´ ì¡°ì¹˜ë¥¼ í•˜ì§€ ì•Šì€ 5ë“±ê¸‰ ì°¨ëŸ‰ì˜ ì„œìš¸ ì¤‘ì‹¬ë¶€ (ì˜› í•œì–‘ë„ì„±ë‚´ë¶€) ì§„ì…ì„ ì œí•œí•˜ê³  ìˆìŠµë‹ˆë‹¤. ì´ì— ì§„ì¶œì…ë¡œ 45ê³³ì— ì„¤ì¹˜ëœ ì¹´ë©”ë¼ 119ëŒ€ë¥¼ í†µí•˜ì—¬ ë‹¨ì†ì„ í•˜ëŠ” ì¤‘ì…ë‹ˆë‹¤. ì´ëŸ¬í•œ ë‹¨ì†ì„ ìë™í™”í•˜ê¸° ìœ„í•˜ì—¬ ìë™ì°¨ ì°¨ëŸ‰ì˜ ì¢…ë¥˜ë¥¼ ì¸ì‹ (Recognition) í•˜ëŠ” í”„ë¡œê·¸ë¨ì„ ë§Œë“¤ê³ ì í•©ë‹ˆë‹¤.

<br>
<img src='https://img6.yna.co.kr/photo/cms/2019/12/03/53/PCM20191203000053990_P4.jpg' width=50%>
<br>

## ğŸ“Œ Data
- ëª¨ë¸ì€ ë”¥ëŸ¬ë‹ ëª¨ë¸ë¡œì¨ `torch`ë¥¼ ì‚¬ìš©í•˜ê²Œ ì œê³µë©ë‹ˆë‹¤. 
- í›ˆë ¨ë°ì´í„°(Train Data): 15000ì¥ ì´ë¯¸ì§€ *(ê°ê°ì˜ í´ë˜ìŠ¤ 3000 ì¥ì˜ ì´ë¯¸ì§€ $\times$ 5 í´ë˜ìŠ¤)*  
- í‰ê°€ë°ì´í„°(Test Data): ì•½ 1000ì¥ ì´ë¯¸ì§€ (ê°ê°ì˜ í´ë˜ìŠ¤ 20 ì¥ì˜ ì´ë¯¸ì§€ $\times$ 5 í´ë˜ìŠ¤)* 

## Evaluation
1000ì¥ ì´ë¯¸ì§€ë¥¼ ì˜ ë¶„ë¥˜í•˜ì˜€ëŠ”ì§€ ROC-AUCë¡œ í‰ê°€í•˜ë„ë¡ í•œë‹¤.

## Â ğŸ–¥ ë“œë¼ì´ë¸Œ ì—°ë™ ë° ë””ë ‰í† ë¦¬ ìƒì„±
"""

# Mount
from google.colab import drive
drive.mount('/content/gdrive')

import os 
os.chdir('/content/gdrive/My Drive/Colab Notebooks/') # DataPath ì„¤ì • 
current_path = os.getcwd() # í˜„ì¬ í´ë” ìœ„ì¹˜

# í´ë” ìƒì„± 
main_dir = os.path.join(current_path, 'Application_practice_NNs')
data_path = os.path.join(main_dir,'data')
os.makedirs(main_dir, exist_ok=True) 
os.makedirs(data_path, exist_ok=True)

# ë°ì´í„° ê²½ë¡œ ì„¤ì • 
INIT_PATH = os.path.join(current_path, 'Application_practice_NNs')
os.chdir(INIT_PATH)
print('ì •ìƒì ìœ¼ë¡œ ë””ë ‰í† ë¦¬ê°€ ìƒì„±ë˜ì—ˆìŠµë‹ˆë‹¤. í•˜ë‹¨ ê²½ë¡œì— ë°ì´í„°ë¥¼ ë„£ì–´ì£¼ì„¸ìš”.')
print('INIT_PATH:', INIT_PATH)

"""## ğŸ–¥ ë°ì´í„°ì…‹ ì—…ë¡œë“œ"""

# Commented out IPython magic to ensure Python compatibility.
# # ì••ì¶• í•´ì œ --> 10 ë¶„ ì†Œìš” 
# %%capture
# if not os.path.exists(os.path.join(INIT_PATH,'dataset')):
#     !tar -xvf dataset.tar.gz

"""## ğŸ–¥ ì••ì¶•í™•ì¸"""

from glob import glob

train = glob('./dataset/train/*.jpg')
train_json = glob('./dataset/train/*.json')
test = glob('./dataset/test/*.jpg')

if (len(train) == len(train_json)) and (len(test) == 995):
    print('|INFO| unzip succeed')
else:
    print('|INFO| unzip failed')

"""## ğŸ–¥ Visualize the dataset"""

import re 
import os 
from random import sample
import matplotlib.pyplot as plt
plt.rc('font', family='NanumBarunGothic') 

regex = r'[A-Za-z]+'

eng_dict = {'motorcycle' : 'ì˜¤í† ë°”ì´',
            'concrete' : 'ë ˆë¯¸ì½˜',
            'bus' : 'ë²„ìŠ¤',
            'benz' : 'ë²¤ì¸ ',
            'suv':'SUV'
            }

plt.figure(figsize=(12,12))
for idx, (file_path) in enumerate(sample(train,16)):
    plt.subplot(4,4, idx+1)
    img = plt.imread(file_path)
    plt.imshow(img)
    
    
    basename = os.path.basename(file_path).split('.')[0]
    label = re.findall(regex,basename)[0]
    # plt.title(eng_dict[label])
    plt.title(label)
    plt.axis('off')
plt.tight_layout()
plt.show()

"""## ğŸ–¥ Import """

# import
import re 
import os
import PIL
import sys
import time
import random
import logging
import numpy as np
import pandas as pd
from datetime import datetime
from glob import glob
from tqdm import tqdm

# torch 
import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import Dataset, DataLoader

import torchvision 
from torchvision import models
import torchvision.transforms as transforms

from torchsummary import summary

class AttributeDict(dict):
    def __init__(self):
        self.__dict__ = self
        
class ConfigTree:
    def __init__(self):
        self.DATASET = AttributeDict()
        self.SYSTEM = AttributeDict()
        self.TRAIN = AttributeDict()
        self.MODEL = AttributeDict()
        self.TEST = AttributeDict()

def print_overwrite(step, total_step, loss, acc, operation):
    sys.stdout.write('\r')
    if operation == 'train':
        sys.stdout.write(f"Train Steps: {step}/{total_step} | Loss: {loss:.4f} | Acc: {acc*100.:.2f} %")   
    else:
        sys.stdout.write(f"Valid Steps: {step}/{total_step} | Loss: {loss:.4f} | Acc: {acc*100.:.2f} %")
    sys.stdout.flush()
    

def get_augmentation(size=224, use_flip=True, use_color_jitter=False, use_gray_scale=False, use_normalize=False):
    resize_crop = transforms.RandomResizedCrop(size=size)
    random_flip = transforms.RandomHorizontalFlip(p=0.5)
    color_jitter = transforms.RandomApply([
        transforms.ColorJitter(0.8, 0.8, 0.8, 0.2)
       
    ], p=0.8)
    gray_scale = transforms.RandomGrayscale(p=0.2)
    normalize = transforms.Normalize((0.485, 0.456, 0.406), (0.229, 0.224, 0.225))
    to_tensor = transforms.ToTensor()
    
    transforms_array = np.array([resize_crop, random_flip, color_jitter, gray_scale, to_tensor, normalize])
    transforms_mask = np.array([True, use_flip, use_color_jitter, use_gray_scale, True, use_normalize])
    
    transform = transforms.Compose(transforms_array[transforms_mask])
    
    return transform


def encoding_name(filename):
    return os.path.basename(filename).split('.')[0]

def submmision(config,file_name,predictions):
    
    PATH = config.SYSTEM.SAVE_DIR
    TEAM_NAME = config.SYSTEM.TEAM_NAME.replace(" ","") # ê³µë°±ì œê±°
    
    # Make output directory
    SAVE_PATH = os.path.join(PATH, "output")
    os.makedirs(SAVE_PATH, exist_ok=True)

    today = datetime.now().strftime("%Y%m%d_%H%M%S")
    csv_name = os.path.join(SAVE_PATH, f'{TEAM_NAME}_{today}.csv')
    
    
    data = np.stack([np.array(file_name),np.array(predictions)],axis=1)
    
    
    # ê²°ê³¼ íŒŒì¼ ì €ì¥ 
    submmision = pd.DataFrame(data=data, columns=['encoded_name','label'])
    submmision['encoded_name'] = submmision['encoded_name'].apply(encoding_name)
    submmision.to_csv(csv_name,index=None)
    
    display(submmision)
 
    print(f'|INFO| DATE: {today}')
    print(f'|INFO| ì œì¶œ íŒŒì¼ ì €ì¥ ì™„ë£Œ: {csv_name}')
    print('í•˜ë‹¨ ì£¼ì†Œì—ã…” ì ‘ì†í•˜ì—¬ ìºê¸€Leaderboard ì— ì—…ë¡œë“œ í•´ì£¼ì„¸ìš”.')
    print('https://www.kaggle.com/t/16531420f61345978c490712a7a5212b')

"""## Create dataset class"""

import json
from PIL import Image
from pathlib import Path


class VehicleDataset(Dataset):
    def __init__(self, cfg, mode, transform):
        self.data_root = os.path.join(cfg.DATASET.ROOT, mode)
        self.mode = mode
        self.transform = transform
        self.images = sorted(glob(self.data_root + '/*.jpg'))
        if mode == 'train':
            self.annotations = sorted(glob(self.data_root + '/*.json'))

    def __len__(self):
        if self.mode == 'train':
            assert len(self.images) == len(
                self.annotations), "# of image files and # of json files do not match"
        return len(self.images)

    def __getitem__(self, index):
        image = self.images[index]
        image = Image.open(image)
        image = self.transform(image)

        if self.mode == 'train':
            annotation = self.annotations[index]
            label = self.get_gt(annotation)
            return image, label
        else:
            return image

    def get_gt(self, json_file):
        label_dict = {
            'motorcycle': 0,
            'concrete': 1,
            'bus': 2,
            'benz': 3,
            'suv': 4
        }

        json_file = Path(json_file)
        with open(json_file, 'r') as f:
            annotation = json.load(f)
        gt_class = label_dict[(annotation['label'])]

        return gt_class

"""<hr style="height:3px;border:none;color:#5F71F7;background-color:#5F71F7;" />

ì—¬ê¸°ì„œ ë¶€í„° ì‹œì‘í•˜ì‹œë©´ ë˜ê² ìŠµë‹ˆë‹¤.

## ğŸ›  Configurations
"""

config = ConfigTree()
config.DATASET.ROOT = "./dataset/"  # data path
config.DATASET.NUM_CLASSES = 5  # ë¶„ë¥˜í•´ì•¼ í•˜ëŠ” í´ë˜ìŠ¤ ì¢…ë¥˜ì˜ ìˆ˜

config.SEED = 2  # seed num


####################################
# ë³€ê²½ í•  ë¶€ë¶„
####################################

config.SYSTEM.GPU = 0  # GPU ë²ˆí˜¸
config.SYSTEM.PRINT_FREQ = 2  # ë¡œê·¸ë¥¼ í”„ë¦°íŠ¸í•˜ëŠ” ì£¼ê¸°
config.SYSTEM.TEAM_NAME = 'RHS' # text.replace(" ","")
config.SYSTEM.SAVE_DIR = './save_csv' # ëª¨ë¸ íŒŒë¼ë¯¸í„°ê°€ ì €ì¥ë˜ëŠ” ìœ„ì¹˜
config.SYSTEM.SAVE_CHECKPOINT = './checkpoint' # ëª¨ë¸ íŒŒë¼ë¯¸í„°ê°€ ì €ì¥ë˜ëŠ” ìœ„ì¹˜

# hyperparameter of experiment
config.TRAIN.EPOCH = 1  # total training epoch
config.TRAIN.BATCH_SIZE = 4
config.TRAIN.BASE_LR = 1e-1
config.TRAIN.WEIGHT_DECAY =1e-1
config.TRAIN.VALID_RATIO = 0.2
config.TRAIN.OPTIM = 'SGD'
config.TRAIN.MODEL = 'vgg11'

# ì‚¬ìš©í•  augmentation
config.TRAIN.AUGMENTATION = {'size': 112, 'use_flip': False, 'use_color_jitter': False,
                             'use_gray_scale': False, 'use_normalize': False}  
config.TEST.AUGMENTATION = {'size': 224, 'use_normalize': False}

# seed fix 
seed = 42
random.seed(seed)
torch.manual_seed(seed)
np.random.seed(seed)

# GPU allocation 
device = torch.device(f'cuda:{config.SYSTEM.GPU}' if torch.cuda.is_available() else 'cpu')
if device == 'cuda':
    torch.cuda.set_device(device)
    print ('Current cuda device ', torch.cuda.current_device()) # check
    with torch.cuda.device(f'cuda:{config.SYSTEM.GPU}'):
        torch.cuda.empty_cache()
    torch.cuda.manual_seed(seed)
    torch.backends.cudnn.deterministic = True
    torch.backends.cudnn.benchmark = False

"""## Train"""

transform = get_augmentation(**config.TRAIN.AUGMENTATION)

dataset = VehicleDataset(config,'train',transform)

len_valid_set = int(config.TRAIN.VALID_RATIO*len(dataset))
len_train_set = len(dataset) - len_valid_set
print("The length of Train set is {}".format(len_train_set))
print("The length of Valid set is {}".format(len_valid_set))

train_dataset , valid_dataset,  = torch.utils.data.random_split(dataset , [len_train_set, len_valid_set])

train_loader = torch.utils.data.DataLoader(train_dataset,
                                           batch_size=config.TRAIN.BATCH_SIZE,
                                           shuffle=True
                                           )

valid_loader = torch.utils.data.DataLoader(valid_dataset,
                                          batch_size=config.TRAIN.BATCH_SIZE,
                                          shuffle=True
                                           )

"""## Testing the shape of input data"""

images, landmarks = next(iter(train_loader))
print(images.shape)

"""## Define the model"""

model_arch=config.TRAIN.MODEL
use_pretrained=False
num_classes=5

model = models.__dict__[model_arch]()

in_features = model.classifier[6].in_features
model.classifier[6] = nn.Linear(in_features, num_classes)
model = model.to(device)

summary(model,(3, 224, 224))

# update_params = [p for p in model.parameters() if p.requires_grad]
optimizer = optim.__dict__[config.TRAIN.OPTIM]
optimizer = optimizer(model.parameters(), lr=config.TRAIN.BASE_LR, weight_decay=config.TRAIN.WEIGHT_DECAY)
criterion = nn.CrossEntropyLoss()

num_epochs = config.TRAIN.EPOCH

loss_min = np.inf
start_time = time.time()
for epoch in range(1,num_epochs+1):
    
    loss_train = 0
    loss_valid = 0
    running_loss = 0
    
    acc_train = 0
    acc_valid = 0
    running_acc = 0
    
    model.train()
    for step , (images, targets) in enumerate(train_loader):
    
        images, targets = images.to(device), targets.to(device)
        
        predictions = model(images)
        
        # clear all the gradients before calculating them
        optimizer.zero_grad()
        
        # find the loss for the current step
        loss_train_step = criterion(predictions, targets).to(torch.float32)
        
        # calculate the gradients
        loss_train_step.backward()
        
        # update the parameters
        optimizer.step()
        
        loss_train += loss_train_step.item()
        running_loss = loss_train/(step+1)
        
        acc_train += predictions.argmax(-1).eq(targets).float().mean()
        running_acc = acc_train/(step+1)
        
        if step % config.SYSTEM.PRINT_FREQ == 0 :
            print_overwrite(step, len(train_loader), running_loss, running_acc, 'train')
        
    model.eval() 
    with torch.no_grad():
        for step , (images, targets) in enumerate(valid_loader):
            images, targets = images.to(device), targets.to(device)
            
            predictions = model(images)

            # find the loss for the current step
            loss_valid_step = criterion(predictions, targets).to(torch.float32)

            loss_valid += loss_valid_step.item()
            running_loss = loss_valid/(step+1)

            acc_valid += predictions.argmax(-1).eq(targets).float().mean()
            running_acc = acc_valid/(step+1)
            
            if step % config.SYSTEM.PRINT_FREQ == 0:
                print_overwrite(step, len(valid_loader), running_loss, running_acc, 'valid')
    
    loss_train /= len(train_loader)
    loss_valid /= len(valid_loader)
    
    acc_train /= len(train_loader)
    acc_valid /= len(valid_loader)
    
    eta = time.time()-start_time
    epoch_eta = time.strftime('%H:%M:%S', time.gmtime(eta))
    print('\n\n'+'-'*90) 
    print('ETA: {} | Epoch: {}/{} | [Train] Loss: {:.4f}, Acc: {:.2f} % | [Valid] Loss: {:.4f}, Acc: {:.2f} %'
      .format(epoch_eta, epoch, num_epochs, loss_train,acc_train*100., loss_valid, acc_valid*100.))
    print('-'*90)
    
    if loss_valid < loss_min:
        loss_min = loss_valid
        torch.save(model.state_dict(), './vehicle_recognition.pth') 
        print("\nMinimum Validation Loss of {:.4f} & Acc of {:.2f} % at epoch {}/{}".format(loss_min,acc_valid*100., epoch, num_epochs))
        print('Model Saved\n')
        
print('Training Complete')
print("Total Elapsed Time : {}".format(time.strftime('%H:%M:%S', time.gmtime(time.time()-start_time))))

"""## Submission

### Predict on Test Images
"""

test_transform = get_augmentation(**config.TEST.AUGMENTATION)
test_dataset = VehicleDataset(config,'test', test_transform)
test_loader = torch.utils.data.DataLoader(test_dataset,
                                          batch_size=4,
                                           )

start_time = time.time()

predictions = []

with torch.no_grad():
    
    best_network = model
    best_network.load_state_dict(torch.load('./vehicle_recognition.pth')) 
    best_network.to(device)
    best_network.eval()

    print('Total number of test images: {}'.format(len(test_loader)))
    for batch_idx, images in tqdm(enumerate(test_loader)):
        images = images.to(device)
        
        outputs = best_network(images)
        pred = outputs.argmax(-1).to('cpu').tolist()
        predictions.extend(pred)

end_time = time.time()
print('Testing Complete')
print("Total Elapsed Time : {}".format(time.strftime('%H:%M:%S', time.gmtime(time.time()-start_time))))

submmision(config, test_loader.dataset.images, predictions)

